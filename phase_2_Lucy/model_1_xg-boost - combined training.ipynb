{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import datetime\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "save_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/feature/'\r\n",
    "wp1 = pd.read_csv(save_address+'wp1.csv')\r\n",
    "wp2 = pd.read_csv(save_address+'wp2.csv')\r\n",
    "wp3 = pd.read_csv(save_address+'wp3.csv')\r\n",
    "wp4 = pd.read_csv(save_address+'wp4.csv')\r\n",
    "wp5 = pd.read_csv(save_address+'wp5.csv')\r\n",
    "wp6 = pd.read_csv(save_address+'wp6.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for wp in [wp1,wp2,wp3,wp4,wp5,wp6]:\r\n",
    "    wp.index = wp['date']\r\n",
    "    wp.drop(columns='date',inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wp1[0:2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_X(df):\r\n",
    "    return df.iloc[:, 1:]\r\n",
    "\r\n",
    "def get_y(df, name='wp1'):\r\n",
    "    return df[name]\r\n",
    "\r\n",
    "def get_X_test(df):\r\n",
    "    return df.iloc[:, 0:]\r\n",
    "\r\n",
    "\r\n",
    "def data_test_split(df, model_address, start_date=\"2009-07-01 00:00:00\", end_date=\"2010-12-31 23:00:00\", name='wp1'):\r\n",
    "    '''\r\n",
    "    resplit data & test from full data\r\n",
    "    '''\r\n",
    "    train = df[df['train'] == 1].sort_values(by='date')\r\n",
    "    test = df[df['train'] != 1].sort_values(by='date')\r\n",
    "    train = train.drop(columns=['train'])\r\n",
    "    test = test.drop(columns=['train', name])\r\n",
    "    train = train.loc[start_date:end_date]\r\n",
    "    train.to_csv(model_address+'training_data_{}.csv'.format(name))\r\n",
    "    X_train = get_X(train)\r\n",
    "    y_train = get_y(train, name)\r\n",
    "    X_forecast = get_X_test(test)\r\n",
    "    return X_train, y_train, X_forecast ,test # train, test,\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "\r\n",
    "def split_data(X, y):\r\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "        X, y, test_size=0.2, random_state=177)\r\n",
    "    return X_train, X_test, y_train, y_test\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGBOOST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import xgboost as xgb\r\n",
    "import model1\r\n",
    "from numba import jit, cuda"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tune_parameter = False\r\n",
    "grid_search = False\r\n",
    "\r\n",
    "#my new params are: {'colsample_bytree': 0.8, 'max_depth': 11, 'min_child_weight': 8, 'eval_metric': 'mae', 'subsample': 0.8, 'colsample': 0.8, 'eta': 0.05}\r\n",
    "params = {\r\n",
    "        'colsample_bytree': 0.9, #tested from 0.8 -0.9\r\n",
    "          'max_depth': 11,  #tested from 4 -14\r\n",
    "          'min_child_weight': 9,  #8 or 9 is good\r\n",
    "          'eval_metric': 'mae',\r\n",
    "          'subsample': 0.8, \r\n",
    "          'colsample': 0.9,\r\n",
    "          'eta': 0.05}\r\n",
    "\r\n",
    "# params['gpu_id'] = 1\r\n",
    "# params[\"predictor\"]= \"gpu_predictor\"\r\n",
    "# params['tree_method'] = 'exact'\r\n",
    "\r\n",
    "num_boost_round = 2000\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FORECAST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prediction(X_forecast, test, model, name):\r\n",
    "    X_forecast = xgb.DMatrix(data=X_forecast)\r\n",
    "    df_predictions = pd.DataFrame({\r\n",
    "        'date': test.index,\r\n",
    "        name: model.predict(X_forecast),\r\n",
    "    })\r\n",
    "    save_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase 2 model/Lucy/result/'\r\n",
    "    df_predictions.to_csv(save_address+\"pred_{}.csv\".format(name))\r\n",
    "    return df_predictions\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Additional Feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "mms = MinMaxScaler()\r\n",
    "\r\n",
    "\r\n",
    "def kMeansRes(scaled_data, k, alpha_k=0.02):\r\n",
    "    inertia_o = np.square((scaled_data - scaled_data.mean(axis=0))).sum()\r\n",
    "    # fit k-means\r\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(scaled_data)\r\n",
    "    scaled_inertia = kmeans.inertia_ / inertia_o + alpha_k * k\r\n",
    "    return scaled_inertia\r\n",
    "\r\n",
    "\r\n",
    "def chooseBestKforKMeans(scaled_data, k_range):\r\n",
    "    ans = []\r\n",
    "    for k in k_range:\r\n",
    "        scaled_inertia = kMeansRes(scaled_data, k)\r\n",
    "        ans.append((k, scaled_inertia))\r\n",
    "    results = pd.DataFrame(ans, columns=['k', 'Scaled Inertia']).set_index('k')\r\n",
    "    best_k = results.idxmin()[0]\r\n",
    "    return best_k, results\r\n",
    "\r\n",
    "def i_want_graph(results):\r\n",
    "    plt.figure(figsize=(7, 4))\r\n",
    "    plt.plot(results, 'o')\r\n",
    "    plt.title('Adjusted Inertia for each K')\r\n",
    "    plt.xlabel('K')\r\n",
    "    plt.ylabel('Adjusted Inertia')\r\n",
    "    plt.xticks(range(1, 10, 1))\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "@jit   \r\n",
    "def add_cluster(train,test):\r\n",
    "\r\n",
    "    k_range = range(1, 10)\r\n",
    "\r\n",
    "    best_k, results = chooseBestKforKMeans(mms.fit_transform(\r\n",
    "        train[['u', 'v', 'ws', 'wd']]), k_range)\r\n",
    "    print('ok..my best k is ...',best_k)\r\n",
    "\r\n",
    "    #i_want_graph(results)\r\n",
    "\r\n",
    "    #print(\"hey, i override my best_k\")\r\n",
    "    #best_k = 4\r\n",
    "    \r\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=77).fit(mms.fit_transform(\r\n",
    "        train[['u', 'v', 'ws', 'wd']]))\r\n",
    "    array1 = kmeans.labels_\r\n",
    "    array2=kmeans.predict(mms.fit_transform(test[['u', 'v', 'ws', 'wd']]))\r\n",
    "    \r\n",
    "    array1=array1.tolist()\r\n",
    "    array2=array2.tolist()\r\n",
    "\r\n",
    "    train['my_cluster'] = pd.Series(array1, index=train.index)\r\n",
    "    test['my_cluster'] = pd.Series(array2, index=test.index)\r\n",
    "    return train, test\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_dummy(data, col):\r\n",
    "    data = pd.get_dummies(data, columns=[col])\r\n",
    "    return data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "\r\n",
    "def scale_features(df, scaler='StandardScaler'):\r\n",
    "    columns = df.columns\r\n",
    "    if scaler == \"MinMaxScaler\":\r\n",
    "            scaler = MinMaxScaler(feature_range=(-1, 1))\r\n",
    "    else:\r\n",
    "            scaler = \"StandardScaler\"\r\n",
    "            scaler = StandardScaler(feature_range=(0, 1))\r\n",
    "\r\n",
    "    df[columns] = scaler.fit_transform(df[columns])\r\n",
    "\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RUNNNNNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#from pandas.io.json import json_normalize\r\n",
    "import xgboost as xgb\r\n",
    "from datetime import date\r\n",
    "import json\r\n",
    "import gc\r\n",
    "\r\n",
    "from timeit import default_timer as timer \r\n",
    "\r\n",
    "#param_list = pd.DataFrame()\r\n",
    "prediction = pd.DataFrame()\r\n",
    "start_date = \"2009-07-01 00:00:00\"\r\n",
    "end_date = \"2010-12-31 23:00:00\"\r\n",
    "\r\n",
    "save_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/result/'\r\n",
    "model_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/model/'\r\n",
    "months_predict = [\r\n",
    "                '201101', \r\n",
    "                '201102', \r\n",
    "                '201103',\r\n",
    "                '201104',\r\n",
    "                '201105',\r\n",
    "                '201106',\r\n",
    "                '201107',\r\n",
    "                '201108',\r\n",
    "                '201109',\r\n",
    "                '201110',\r\n",
    "                '201111',\r\n",
    "                '201112',\r\n",
    "                '201201',\r\n",
    "                '201202',\r\n",
    "                '201203',\r\n",
    "                '201204',\r\n",
    "                '201205',\r\n",
    "                '201206']\r\n",
    "train_end_date = [\r\n",
    "                \"2010-12-31 23:00:00\",\r\n",
    "                \"2011-01-31 23:00:00\",\r\n",
    "                \"2011-02-28 23:00:00\",\r\n",
    "                \"2011-03-31 23:00:00\",\r\n",
    "                \"2011-04-30 23:00:00\",\r\n",
    "                \"2011-05-30 23:00:00\",\r\n",
    "                \"2011-06-30 23:00:00\",\r\n",
    "                \"2011-07-31 23:00:00\",\r\n",
    "                \"2011-08-31 23:00:00\",\r\n",
    "                \"2011-09-30 23:00:00\",\r\n",
    "                \"2011-10-31 23:00:00\",  \r\n",
    "                \"2011-11-30 23:00:00\",\r\n",
    "                \"2011-12-31 23:00:00\",\r\n",
    "                \"2012-01-31 23:00:00\",\r\n",
    "                \"2012-02-29 23:00:00\",\r\n",
    "                \"2012-03-31 23:00:00\",\r\n",
    "                \"2012-04-30 23:00:00\",\r\n",
    "                \"2012-05-31 23:00:00\"]\r\n",
    "\r\n",
    "for year, end_date in zip(months_predict, train_end_date):\r\n",
    "    # for name,df in zip([\"wp1\", \"wp2\", \"wp3\", \"wp4\", \"wp5\", \"wp6\"],[wp1,wp2,wp3,wp4,wp5,wp6]):\r\n",
    "    #     print('------ name is :', name, \"------\")\r\n",
    "    #     #split my merged dataset\r\n",
    "    #     X, y, X_forecast,test = data_test_split(df, model_address=model_address, start_date=start_date, end_date=end_date, name=name)\r\n",
    "    #     #split to train model\r\n",
    "    #     '''we do the modification here'''\r\n",
    "    for name_1, df_1 in zip([\"wp1\", \"wp2\", \"wp3\", \"wp4\", \"wp5\", \"wp6\"], [wp1, wp2, wp3, wp4, wp5, wp6]):\r\n",
    "            X_temp, y_temp, X_forecast_temp,test_temp = data_test_split(\r\n",
    "                df_1, model_address=model_address, start_date=start_date, end_date=end_date, name=name_1)\r\n",
    "            test_temp['wp'] = name_1\r\n",
    "            if name_1 == 'wp1':\r\n",
    "                X_temp['wp'] = name_1\r\n",
    "                X_forecast_temp['wp'] = name_1\r\n",
    "                my_X = X_temp\r\n",
    "                my_y = y_temp\r\n",
    "                my_X_forecast = X_forecast_temp\r\n",
    "                my_test =test_temp\r\n",
    "                print(name_1,'...my X length is...', len(my_X), '...my y length is...', len(my_y))\r\n",
    "            else:\r\n",
    "                X_temp['wp'] = name_1\r\n",
    "                X_forecast_temp['wp'] = name_1\r\n",
    "                my_X = my_X.append(X_temp)\r\n",
    "                my_y = my_y.append(y_temp)\r\n",
    "                my_X_forecast = my_X_forecast.append(X_forecast_temp)\r\n",
    "                my_test = my_test.append(test_temp)\r\n",
    "                print('my X length is..', len(my_X),', my y length is..', len(my_y))\r\n",
    "                print('my_X_forecast length is..', len(my_X_forecast), ', my_test length is..', len(my_test))\r\n",
    "                del X_temp, y_temp, X_forecast_temp, test_temp\r\n",
    "                gc.collect()\r\n",
    "\r\n",
    "    X = my_X\r\n",
    "    y = my_y\r\n",
    "    X_forecast = my_X_forecast\r\n",
    "    test=my_test\r\n",
    "    ''''''\r\n",
    "    #ADDING FEATURE after merged dataset\r\n",
    "\r\n",
    "    X = create_dummy(X, 'wp')\r\n",
    "    X_forecast = create_dummy(X_forecast, 'wp')\r\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\r\n",
    "    X_forecast.replace([np.inf, -np.inf], np.nan, inplace=True)\r\n",
    "    X=X.fillna(X.mean())\r\n",
    "    X_forecast = X_forecast.fillna(X_forecast.mean())\r\n",
    "\r\n",
    "    #BUG: I don't know why, but normalization is a BAD BAD IDEA! \r\n",
    "    #print('My X null is...', X.isnull().sum().sum(), ', and my max is ',X.max().max())\r\n",
    "    #X = scale_features(X, scaler='MinMaxScaler')\r\n",
    "    #X_forecast = scale_features(X_forecast, scaler='MinMaxScaler')\r\n",
    "\r\n",
    "    #print(os.getcwd())\r\n",
    "    #X.to_csv('what is going on.csv')\r\n",
    "\r\n",
    "    X, X_forecast = add_cluster(X, X_forecast)\r\n",
    "\r\n",
    "    temp_data_path = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/data/intermediate validation/'\r\n",
    "    if year== '201206':\r\n",
    "        X.to_csv(temp_data_path+\"X_{}.csv\".format(year),index=False)\r\n",
    "        X_forecast.to_csv(temp_data_path+\"X_forecast_{}.csv\".format(year),index=False)\r\n",
    "    ''''''\r\n",
    "    print('run model now')\r\n",
    "    ts = time.time()\r\n",
    "    X_train, X_test, y_train, y_test = split_data(X,y)\r\n",
    "    model,param,feat_imp = model1.run_xgb(X_train, X_test, y_train, y_test, params, num_boost_round=num_boost_round,\r\n",
    "                                 tune_parameter=True, grid_search=False, graph=True)\r\n",
    "    te = time.time()                             \r\n",
    "    print('model finished now..',\"%2.2f min\" % ( (te - ts) / 60))\r\n",
    "\r\n",
    "    feat_imp.to_csv(model_address+'my_feature_importance_table.csv')\r\n",
    "    '''\r\n",
    "    Forecast\r\n",
    "    '''\r\n",
    "    X_forecast = xgb.DMatrix(data=X_forecast)\r\n",
    "    df_predictions = pd.DataFrame({\r\n",
    "            'date': test.index,\r\n",
    "            'wp':test['wp'],\r\n",
    "            \"forecast\": model.predict(X_forecast),\r\n",
    "        })\r\n",
    "    model.save_model(model_address+\"model_merged_train_{}.json\".format(year))\r\n",
    "        #df_predictions.to_csv(save_address+\"pred_{}.csv\".format(name))\r\n",
    "        #print(param)\r\n",
    "        #print(type(param)) #This is string type\r\n",
    "        #param = json.loads(param)\r\n",
    "    with open(model_address+\"param_merged_train_{}.json\".format(year), 'w') as f:\r\n",
    "            json.dump(param, f)\r\n",
    "        #param_list = param_list.append(param)\r\n",
    "    # if name =='wp1':\r\n",
    "    #         prediction = df_predictions\r\n",
    "    #     else:\r\n",
    "    #         prediction = prediction.merge(df_predictions,on=['date'])\r\n",
    "    prediction= pd.pivot_table(df_predictions, index=df_predictions.index,\r\n",
    "                   columns='wp', values='forecast', aggfunc=min).reset_index()\r\n",
    "\r\n",
    "    prediction.rename(columns={'date': 'date1'}, inplace=True)\r\n",
    "    data_path = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/data/'\r\n",
    "    test = pd.read_csv(f'{data_path}test.csv')\r\n",
    "    prediction = pd.concat([test, prediction], axis=1)\r\n",
    "    prediction.drop(columns='date1', inplace=True)\r\n",
    "\r\n",
    "    save_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/result/before_submission/'\r\n",
    "    prediction.to_csv(\r\n",
    "    save_address+'predictions_end_date_{}.csv'.format(year), index=False, sep=';')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Take Monthly Result and Combine --> THIS STEP ENSURES THAT NO USE OF FUTURE DATA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_folder(file_folder):\r\n",
    "    \"\"\"\r\n",
    "        Read all files in the folder\r\n",
    "        \"\"\"\r\n",
    "    files = os.listdir(file_folder)\r\n",
    "    df = []\r\n",
    "    for f in files:\r\n",
    "        print(f)\r\n",
    "        my_file = file_folder + \"/\" + f\r\n",
    "        temp = pd.read_csv(my_file, sep=';')\r\n",
    "        temp['file_name']=f\r\n",
    "        df.append(temp)\r\n",
    "    df_full = pd.concat(df, ignore_index=True)\r\n",
    "    return df_full\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_full = read_folder(\r\n",
    "    \"C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/result/before_submission/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_full['file_forecast_month'] = df_full['file_name'].str[21:27]\r\n",
    "df_full['forecast_month'] = df_full['date'].astype(str).str[0:6]\r\n",
    "\r\n",
    "df_full['match'] = np.where(df_full['file_forecast_month']\r\n",
    "                       == df_full['forecast_month'],True,False)\r\n",
    "df_full=df_full[df_full['match']==True]\r\n",
    "prediction = df_full.drop(\r\n",
    "    columns=['file_forecast_month', 'match', 'file_name', 'forecast_month'])\r\n",
    "save_address2 = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/result/submission/'\r\n",
    "prediction.to_csv(\r\n",
    "    save_address2+'predictions_{}.csv'.format(date.today()), index=False, sep=';')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# check model parameters "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "read_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase 2 model/Lucy/result/'\r\n",
    "pred_wp1=pd.read_csv(read_address+\"pred_wp1.csv\")\r\n",
    "pred_wp2=pd.read_csv(read_address+\"pred_wp2.csv\")\r\n",
    "pred_wp3=pd.read_csv(read_address+\"pred_wp3.csv\")\r\n",
    "pred_wp4=pd.read_csv(read_address+\"pred_wp4.csv\")\r\n",
    "pred_wp5=pd.read_csv(read_address+\"pred_wp5.csv\")\r\n",
    "pred_wp6=pd.read_csv(read_address+\"pred_wp6.csv\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "model_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/model/'\r\n",
    "def read_my_json(model_address,my_file):\r\n",
    "    f = open(model_address+my_file)\r\n",
    "    json_file = json.load(f)\r\n",
    "    return json_file\r\n",
    "\r\n",
    "\r\n",
    "param1 = json.loads(read_my_json(\r\n",
    "    model_address, 'param_merged_train_201205.json'))\r\n",
    "param2 = json.loads(read_my_json(\r\n",
    "    model_address, 'param_merged_train_201206.json'))\r\n",
    "# param3 = json.loads(read_my_json(\r\n",
    "#     model_address, 'param_merged_train 201103.json'))\r\n",
    "# param4 = json.loads(read_my_json(\r\n",
    "#     model_address, 'param_merged_train 201104.json'))\r\n",
    "# param5 = json.loads(read_my_json(\r\n",
    "#     model_address, 'param_merged_train 201105.json'))\r\n",
    "# param6 = json.loads(read_my_json(\r\n",
    "#     model_address, 'param_merged_train 201106.json'))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.json_normalize(param1)\r\n",
    "df=df.append(pd.json_normalize(param2))\r\n",
    "df=df.append(pd.json_normalize(param3))\r\n",
    "# df=df.append(pd.json_normalize(param4))\r\n",
    "# df=df.append(pd.json_normalize(param5))\r\n",
    "# df = df.append(pd.json_normalize(param6))\r\n",
    "df=df.reset_index(drop=True)\r\n",
    "df.to_csv(model_address + 'my_tuned_params_combined.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcf0cd325fd2ed104d5e10c8733d9bb9734c3777ea511c7650c63c0f2bc36237"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}