{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import datetime\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "data_path = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/data/'\r\n",
    "train = pd.read_csv(f'{data_path}train.csv')\r\n",
    "test = pd.read_csv(f'{data_path}test.csv')\r\n",
    "wp1 = pd.read_csv(f'{data_path}wp1.csv')\r\n",
    "wp2 = pd.read_csv(f'{data_path}wp2.csv')\r\n",
    "wp3 = pd.read_csv(f'{data_path}wp3.csv')\r\n",
    "wp4 = pd.read_csv(f'{data_path}wp4.csv')\r\n",
    "wp5 = pd.read_csv(f'{data_path}wp5.csv')\r\n",
    "wp6 = pd.read_csv(f'{data_path}wp6.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "def get_df_name(df):\r\n",
    "    name = [x for x in globals() if globals()[x] is df][0]\r\n",
    "    return name\r\n",
    "\r\n",
    "for df in [train,test,wp1,wp2,wp3,wp4,wp5,wp6]:\r\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%Y%m%d%H')\r\n",
    "    print(\"NaN in Dataset: \",get_df_name(df), df.isnull().values.any())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NaN in Dataset:  train False\n",
      "NaN in Dataset:  test False\n",
      "NaN in Dataset:  wp1 True\n",
      "NaN in Dataset:  wp2 True\n",
      "NaN in Dataset:  wp3 True\n",
      "NaN in Dataset:  wp4 True\n",
      "NaN in Dataset:  wp5 True\n",
      "NaN in Dataset:  wp6 True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "train['train'] = 1\r\n",
    "train = train.reset_index()\r\n",
    "test = test.reset_index()\r\n",
    "data_full = train.append(test)\r\n",
    "data_full = data_full.sort_values(by='date')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "def process_wp_file(wp):\r\n",
    "    wp['contain_na'] = wp['u'].isna()\r\n",
    "    wp['date_f'] = wp['date']+pd.to_timedelta(wp['hors'], unit='h')\r\n",
    "    wp = wp.sort_values(by=['contain_na','date_f', 'date'], ascending=(True, True, False), na_position='last')\r\n",
    "    wp.drop_duplicates(subset=['date_f'],keep='first',inplace=True)\r\n",
    "    wp['date']=wp['date_f']\r\n",
    "    wp=wp.drop(columns=['date_f','hors','contain_na'])\r\n",
    "    return wp\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "wp1 = process_wp_file(wp1)\r\n",
    "wp1 = pd.merge(data_full, wp1, on=['date'], how='left').drop(\r\n",
    "    columns=['index','wp2', 'wp3', 'wp4', 'wp5', 'wp6'])\r\n",
    "wp2 = process_wp_file(wp2)\r\n",
    "wp2 = pd.merge(data_full, wp2, on=['date'], how='left').drop(\r\n",
    "    columns=['index','wp1', 'wp3', 'wp4', 'wp5', 'wp6'])\r\n",
    "wp3 = process_wp_file(wp3)\r\n",
    "wp3 = pd.merge(data_full, wp3, on=['date'], how='left').drop(\r\n",
    "    columns=['index', 'wp2', 'wp1', 'wp4', 'wp5', 'wp6'])\r\n",
    "wp4 = process_wp_file(wp4)\r\n",
    "wp4 = pd.merge(data_full, wp4, on=['date'], how='left').drop(\r\n",
    "    columns=['index', 'wp2', 'wp3', 'wp1', 'wp5', 'wp6'])\r\n",
    "wp5 = process_wp_file(wp5)\r\n",
    "wp5 = pd.merge(data_full, wp5, on=['date'], how='left').drop(\r\n",
    "    columns=['index', 'wp2', 'wp3', 'wp4', 'wp1', 'wp6'])\r\n",
    "wp6 = process_wp_file(wp6)\r\n",
    "wp6 = pd.merge(data_full, wp6, on=['date'], how='left').drop(\r\n",
    "    columns=['index', 'wp2', 'wp3', 'wp4', 'wp5', 'wp1'])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# def mask_na(wp,option=1):\r\n",
    "#     if option ==1:\r\n",
    "#        wp[['u', 'v', 'ws', 'wd']]=wp[['u', 'v', 'ws', 'wd']].fillna(method='ffill', inplace=True)\r\n",
    "#        return wp\r\n",
    "#     elif option ==2:\r\n",
    "#         wp.interpolate()\r\n",
    "#         return wp\r\n",
    "#     elif option ==3:\r\n",
    "#         wp[['u', 'v', 'ws', 'wd']]=wp[['u', 'v', 'ws', 'wd']].fillna(method='bfill', inplace=True)\r\n",
    "#         return wp\r\n",
    "#     else:\r\n",
    "#         from impyte import impyte\r\n",
    "#         wp['date'] = wp['date'].astype(str)\r\n",
    "#         imp = impyte.Impyter(wp)\r\n",
    "#         wp= imp.impute(estimator='rf', multi_nans=True)\r\n",
    "#         wp['date'] = pd.to_datetime(wp['date'], format='%Y%m%d%H')\r\n",
    "#         return wp\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "fill_method='bfill'\r\n",
    "columns = ['u', 'v', 'ws', 'wd']\r\n",
    "wp1[columns] = wp1[columns].fillna(method=fill_method)\r\n",
    "wp2[columns] = wp2[columns].fillna(method=fill_method)\r\n",
    "wp3[columns] = wp3[columns].fillna(method=fill_method)\r\n",
    "wp4[columns] = wp4[columns].fillna(method=fill_method)\r\n",
    "wp5[columns] = wp5[columns].fillna(method=fill_method)\r\n",
    "wp6[columns] = wp6[columns].fillna(method=fill_method)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "save_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/data/'\r\n",
    "wp1.to_csv(save_address+'full_data_wp1.csv', index=False)\r\n",
    "wp2.to_csv(save_address+'full_data_wp2.csv', index=False)\r\n",
    "wp3.to_csv(save_address+'full_data_wp3.csv', index=False)\r\n",
    "wp4.to_csv(save_address+'full_data_wp4.csv', index=False)\r\n",
    "wp5.to_csv(save_address+'full_data_wp5.csv', index=False)\r\n",
    "wp6.to_csv(save_address+'full_data_wp6.csv', index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "save_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/data/'\r\n",
    "wp1= pd.read_csv(save_address+'full_data_wp1.csv')\r\n",
    "wp2 = pd.read_csv(save_address+'full_data_wp2.csv')\r\n",
    "wp3 = pd.read_csv(save_address+'full_data_wp3.csv')\r\n",
    "wp4 = pd.read_csv(save_address+'full_data_wp4.csv')\r\n",
    "wp5 = pd.read_csv(save_address+'full_data_wp5.csv')\r\n",
    "wp6 = pd.read_csv(save_address+'full_data_wp6.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "#MInmax Scaler: This Scaler responds well if the standard deviation is small and when a distribution is not Gaussian. This Scaler is sensitive to outliers.\r\n",
    "#Standar Scaler: The Standard Scaler assumes data is normally distributed within each feature and scales them such that the distribution centered around 0, with a standard deviation of 1.\r\n",
    "\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "\r\n",
    "def scale_features(df,scaler='StandardScaler'):\r\n",
    "    if scaler == \"MinMaxScaler\":\r\n",
    "        scaler = MinMaxScaler(feature_range =(0, 1))\r\n",
    "    else:\r\n",
    "        scaler = \"StandardScaler\"\r\n",
    "        scaler = StandardScaler(feature_range =(0, 1))\r\n",
    "\r\n",
    "    df[['u','v','ws','wd']] = scaler.fit_transform(df[['u','v','ws','wd']])\r\n",
    "    \r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "from tsfresh.feature_extraction import extract_features, ComprehensiveFCParameters\r\n",
    "\r\n",
    "def add_ts_features(df):\r\n",
    "    extraction_settings = ComprehensiveFCParameters()\r\n",
    "\r\n",
    "    X = extract_features(df, column_id='id', column_sort='time',\r\n",
    "                     default_fc_parameters=extraction_settings,\r\n",
    "                     impute_function=impute)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "def add_features(df):\r\n",
    "    wv = df['ws']\r\n",
    "    wd_rad = df['wd']*np.pi / 180\r\n",
    "\r\n",
    "    # Calculate the wind x and y components.\r\n",
    "    df['Wx'] = wv*np.cos(wd_rad)\r\n",
    "    df['Wy'] = wv*np.sin(wd_rad)\r\n",
    "    return df\r\n",
    "\r\n",
    "def add_features2(df):\r\n",
    "    #df['year']=pd.DatetimeIndex(df['date']).year\r\n",
    "    #df['month']=pd.DatetimeIndex(df['date']).month\r\n",
    "    # df['day']=pd.DatetimeIndex(df['date']).day\r\n",
    "    # df['hour']=pd.DatetimeIndex(df['date']).hour\r\n",
    "    # df['dayofweek_num']=pd.DatetimeIndex(df['date']).dayofweek  \r\n",
    "    hour = 60*60\r\n",
    "    day = 24*60*60\r\n",
    "    month = (30)*day\r\n",
    "    year = (365.2425)*day\r\n",
    "    \r\n",
    "    date_time = pd.DatetimeIndex(df['date'])\r\n",
    "    timestamp_s = date_time.map(datetime.datetime.timestamp)\r\n",
    "\r\n",
    "    #df['Hour sin'] = np.sin( timestamp_s* (2 * np.pi / hour))\r\n",
    "    #df['Hour cos'] = np.cos( timestamp_s * (2 * np.pi / hour))\r\n",
    "    df['Day sin'] = np.sin( timestamp_s* (2 * np.pi / day))\r\n",
    "    df['Day cos'] = np.cos( timestamp_s * (2 * np.pi / day))\r\n",
    "    #df['Month sin'] = np.sin( timestamp_s* (2 * np.pi / month))\r\n",
    "    #df['Month cos'] = np.cos( timestamp_s * (2 * np.pi / month))\r\n",
    "    df['Year sin'] = np.sin( timestamp_s * (2 * np.pi / year))\r\n",
    "    df['Year cos'] = np.cos( timestamp_s * (2 * np.pi / year))\r\n",
    "    return df\r\n",
    "    \r\n",
    "def add_features3(data):\r\n",
    "    #data[['u','v','ws','wd']] = np.abs(data[['u','v','ws','wd']])\r\n",
    "    data.index=data['date']\r\n",
    "    data['lag_1_u'] = data['u'].shift(1)\r\n",
    "    data['lag_1_v'] = data['v'].shift(1)\r\n",
    "    data['lag_1_ws'] = data['ws'].shift(1)\r\n",
    "    data['lag_-1_u'] = data['u'].shift(-1)\r\n",
    "    data['lag_-1_v'] = data['v'].shift(-1)\r\n",
    "    data['lag_-1_ws'] = data['ws'].shift(-1)\r\n",
    "\r\n",
    "    # data['lag_1_Wx'] = data['Wx'].shift(1)\r\n",
    "    # data['lag_1_Wy'] = data['Wy'].shift(1)\r\n",
    "    # data['lag_-1_Wx'] = data['Wx'].shift(-1)\r\n",
    "    # data['lag_-1_Wy'] = data['Wy'].shift(-1)\r\n",
    "    #data['lag_1_wd'] = data['wd'].shift(1)\r\n",
    "    #data['lag_-1_wd'] = data['wd'].shift(-1)\r\n",
    "\r\n",
    "    data['expanding_mean_u'] = data['u'].expanding(1,center=True).mean()#,center=True\r\n",
    "    data['expanding_mean_v'] = data['v'].expanding(1,center=True).mean()\r\n",
    "    data['expanding_mean_ws'] = data['ws'].expanding(1,center=True).mean()\r\n",
    "    #data['expanding_mean_Wx'] = data['Wx'].expanding(1,center=True).mean()\r\n",
    "    #data['expanding_mean_Wy'] = data['Wy'].expanding(1,center=True).mean()\r\n",
    "    #data['expanding_mean_wd'] = data['wd'].expanding(2).mean()\r\n",
    "    \r\n",
    "    # data['rolling_mean_ws'] = data['ws'].rolling(window=2,closed='both').mean()\r\n",
    "    # data['rolling_mean_wd'] = data['wd'].rolling(window=2,closed='both').mean()\r\n",
    "    # data['rolling_mean_v'] = data['v'].rolling(window=2,closed='both').mean()\r\n",
    "    # data['rolling_mean_u'] = data['u'].rolling(window=2,closed='both').mean()\r\n",
    "    \r\n",
    "    \r\n",
    "    data['lag_2_u'] = data['u'].shift(2)\r\n",
    "    data['lag_2_v'] = data['v'].shift(2)\r\n",
    "    data['lag_2_ws'] = data['ws'].shift(2)\r\n",
    "    data['lag_-2_u'] = data['u'].shift(-2)\r\n",
    "    data['lag_-2_v'] = data['v'].shift(-2)\r\n",
    "    data['lag_-2_ws'] = data['ws'].shift(-2)\r\n",
    "    \r\n",
    "    # data['rolling_mean_u_1'] = data['u'].rolling(2,closed='both').mean()\r\n",
    "    # data['rolling_mean_v_1'] = data['v'].rolling(2,closed='both').mean()\r\n",
    "    # data['rolling_mean_ws_1'] = data['ws'].rolling(2,closed='both').mean()\r\n",
    "    # data['rolling_mean_wd_1'] = data['wd'].rolling(2,closed='both').mean()\r\n",
    "\r\n",
    "    return data\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "def add_features4(df):\r\n",
    "    \"\"\"Adds Features to DataFrame and Takes Averages for Dates Before train_end_str\"\"\"    \r\n",
    "\r\n",
    "    df['Is_day_part1'] = (pd.DatetimeIndex(df['date']).hour.isin([1,2,3,4,5,6])) *1\r\n",
    "    df['Is_day_part2'] = (pd.DatetimeIndex(df['date']).hour.isin([7,8,9,10,11,12])) *1\r\n",
    "    df['Is_day_part3'] = (pd.DatetimeIndex(df['date']).hour.isin([13,14,15,16,17,18])) *1\r\n",
    "    df['Is_day_part4'] = (pd.DatetimeIndex(df['date']).hour.isin([19,20,21,22,23,24])) *1\r\n",
    "    #df['Is_wknd'] = pd.DatetimeIndex(df['date']).dayofweek // 4 # Fri-Sun are 4-6, Monday is 0 so this is valid\r\n",
    "    \r\n",
    "    #df['Month'] = pd.DatetimeIndex(df['date']).month    \r\n",
    "\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "from sklearn.cluster import KMeans\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "mms = MinMaxScaler()\r\n",
    "\r\n",
    "\r\n",
    "def kMeansRes(scaled_data, k, alpha_k=0.02):\r\n",
    "    inertia_o = np.square((scaled_data - scaled_data.mean(axis=0))).sum()\r\n",
    "    # fit k-means\r\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(scaled_data)\r\n",
    "    scaled_inertia = kmeans.inertia_ / inertia_o + alpha_k * k\r\n",
    "    return scaled_inertia\r\n",
    "\r\n",
    "\r\n",
    "def chooseBestKforKMeans(scaled_data, k_range):\r\n",
    "    ans = []\r\n",
    "    for k in k_range:\r\n",
    "        scaled_inertia = kMeansRes(scaled_data, k)\r\n",
    "        ans.append((k, scaled_inertia))\r\n",
    "    results = pd.DataFrame(ans, columns=['k', 'Scaled Inertia']).set_index('k')\r\n",
    "    best_k = results.idxmin()[0]\r\n",
    "    return best_k, results\r\n",
    "\r\n",
    "def i_want_graph(results):\r\n",
    "    plt.figure(figsize=(7, 4))\r\n",
    "    plt.plot(results, 'o')\r\n",
    "    plt.title('Adjusted Inertia for each K')\r\n",
    "    plt.xlabel('K')\r\n",
    "    plt.ylabel('Adjusted Inertia')\r\n",
    "    plt.xticks(range(1, 10, 1))\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "def add_features5(df):\r\n",
    "\r\n",
    "    df['year'] = pd.DatetimeIndex(df['date']).year\r\n",
    "    k_range = range(1, 10)\r\n",
    "\r\n",
    "    best_k, results = chooseBestKforKMeans(mms.fit_transform(\r\n",
    "        df[df['year'].isin([2009, 2010])][['u', 'v', 'ws', 'wd']]), k_range)\r\n",
    "    print('ok..my best k is ...',best_k)\r\n",
    "\r\n",
    "    #i_want_graph(results)\r\n",
    "\r\n",
    "    #print(\"hey, i override my best_k\")\r\n",
    "    #best_k = 4\r\n",
    "    \r\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=77).fit(mms.fit_transform(\r\n",
    "        df[df['year'].isin([2009, 2010])][['u', 'v', 'ws', 'wd']]))\r\n",
    "    array1 = kmeans.labels_\r\n",
    "    array2=kmeans.predict(mms.fit_transform(df[df['year'].isin(\r\n",
    "        [2011, 2012])][['u', 'v', 'ws', 'wd']]))\r\n",
    "    \r\n",
    "    array1=array1.tolist()\r\n",
    "    array2=array2.tolist()\r\n",
    "    \r\n",
    "    predict = array1 + array2\r\n",
    "    df['my_cluster'] = pd.Series(predict, index=df.index)\r\n",
    "    df = df.drop(columns='year')\r\n",
    "    return df\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "def add_features6(df):\r\n",
    "    df['dff_u_1'] = df['u']-df['u'].shift(1)\r\n",
    "    df['dff_v_1'] = df['v']-df['v'].shift(1)\r\n",
    "    df['dff_ws_1'] = df['ws']-df['ws'].shift(1)\r\n",
    "    #df['dff_wd_1'] = df['wd']-df['wd'].shift(-1)\r\n",
    "    df['dff_u_-1'] = df['u']-df['u'].shift(-1)\r\n",
    "    df['dff_v_-1'] = df['v']-df['v'].shift(-1)\r\n",
    "    df['dff_ws_-1'] = df['ws']-df['ws'].shift(-1)\r\n",
    "    #df['dff_wd_-1'] = df['wd']-df['wd'].shift(-1)\r\n",
    "    df['dff_u_mean'] = df['u']-df['u'].expanding(1, center=True).mean()\r\n",
    "    df['dff_v_mean'] = df['v']-df['v'].expanding(1,center=True).mean()\r\n",
    "    df['dff_ws_mean'] = df['ws']-df['ws'].expanding(1,center=True).mean()\r\n",
    "    #df['dff_wd_mean'] = df['wd']-df['wd'].expanding(1, center=True).mean()\r\n",
    "\r\n",
    "    df['dff_u_1%'] = df['dff_u_1']/df['u']\r\n",
    "    df['dff_v_1%'] = df['dff_v_1']/df['v']\r\n",
    "    df['dff_ws_1%'] = df['dff_ws_1']/df['ws']\r\n",
    "    df['dff_wd_1%'] = (df['wd']-df['wd'].shift(-1))/df['wd']\r\n",
    "    df['dff_u_-1%'] = df['dff_u_-1']/df['u']\r\n",
    "    df['dff_v_-1%'] = df['dff_v_-1']/df['v']\r\n",
    "    df['dff_ws_-1%'] = df['dff_ws_-1']/df['ws']\r\n",
    "    df['dff_wd_-1%'] = (df['wd']-df['wd'].shift(-1))/df['wd']\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Sum_of_squared_distances = []\r\n",
    "# wp1['year'] = pd.DatetimeIndex(wp1['date']).year\r\n",
    "# K = range(1, 10)\r\n",
    "# for num_clusters in K:\r\n",
    "#  kmeans = KMeans(n_clusters=num_clusters)\r\n",
    "#  kmeans.fit(wp1[wp1['year'].isin([2011, 2012])][['u', 'v', 'ws', 'wd']])\r\n",
    "#  Sum_of_squared_distances.append(kmeans.inertia_)\r\n",
    "# plt.plot(K, Sum_of_squared_distances)\r\n",
    "# plt.xlabel('Values of K')\r\n",
    "# plt.ylabel('Sum of squared distances/Inertia')\r\n",
    "# plt.title('Elbow Method For Optimal k')\r\n",
    "# plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "def add_all_features(df):\r\n",
    "    #df=scale_features(df)\r\n",
    "    #df=add_features(df)\r\n",
    "    df=add_features2(df)  \r\n",
    "    df=add_features3(df)\r\n",
    "    #df=add_features4(df)\r\n",
    "    df=add_features5(df)\r\n",
    "    df = add_features6(df)\r\n",
    "    return df\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "wp1=add_all_features(wp1)\r\n",
    "wp2=add_all_features(wp2)\r\n",
    "wp3=add_all_features(wp3)\r\n",
    "wp4=add_all_features(wp4)\r\n",
    "wp5=add_all_features(wp5)\r\n",
    "wp6=add_all_features(wp6)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ok..my best k is ... 8\n",
      "hey, i override my best_k\n",
      "ok..my best k is ... 8\n",
      "hey, i override my best_k\n",
      "ok..my best k is ... 7\n",
      "hey, i override my best_k\n",
      "ok..my best k is ... 7\n",
      "hey, i override my best_k\n",
      "ok..my best k is ... 7\n",
      "hey, i override my best_k\n",
      "ok..my best k is ... 6\n",
      "hey, i override my best_k\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "save_address = 'C:/Users/xi-lucy.chen/Documents/GitLab/data_challenge_alc_total/phase_2_Lucy/feature/'\r\n",
    "wp1.to_csv(save_address+'wp1.csv',index=False)\r\n",
    "wp2.to_csv(save_address+'wp2.csv',index=False)\r\n",
    "wp3.to_csv(save_address+'wp3.csv',index=False)\r\n",
    "wp4.to_csv(save_address+'wp4.csv',index=False)\r\n",
    "wp5.to_csv(save_address+'wp5.csv',index=False)\r\n",
    "wp6.to_csv(save_address+'wp6.csv',index=False)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f94b3753c628799450866864c7093af0e90533b360fa82307d357e1f74a9ff1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}